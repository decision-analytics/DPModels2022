{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/bigsem.png\" width=\"40%\" align=\"right\">\n",
    "<img src=\"img/logo_wiwi.png\" width=\"20%\" align=\"left\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "# Dynamic Programming Models in Combinatorial Optimization\n",
    "**Winter Term 2022/23**\n",
    "\n",
    "\n",
    "# 2. Decision Diagrams\n",
    "\n",
    "<img src=\"img/decision_analytics_logo.png\" width=\"17%\" align=\"right\">\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**J-Prof. Dr. Michael RÃ¶mer |  Decision Analytics Group**\n",
    "                                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "from typing import NamedTuple, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from numba.experimental import jitclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview\n",
    "- Review, generic DP models and algorithms\n",
    "- Exact Decision Diagrams\n",
    "- REducing Exact Decision Diagrams\n",
    "- Restricted Decision Diagrams\n",
    "- Branch and Bound based on Decision Diagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: the 0/1 knapsack problem\n",
    "\n",
    "Given \n",
    "- a knapsack with a capacity $W$ \n",
    "- and a set of items, each with a weight $w_i$ and a value $p_i$\n",
    "- determine the the subset of the items to put in the knapsack such that\n",
    "  - the total value of the items in the knapsack is maximal and\n",
    "  - the total weight of the items in the knapsack does not exceed $W$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example:**\n",
    "\n",
    "<img src=\"./img/greedy/07.png\" width=\"20%\" align=\"right\">\n",
    "\n",
    "Assume you are a thief and you are about to steal the three items depicted below from an appartment. However, your backpack can only fit 35 lbs. Which items should you take?\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./img/greedy/08.png\" width=\"40%\" align=\"left \">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Greedy Approach for the Knapsack Problem\n",
    "\n",
    "- start with some item: If it (still) fits in the backpack, put it in the backpack\n",
    "- repeat for the remaining items\n",
    "\n",
    "..you never take out an item once it has been packed in the knapsack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def greedy_knapsack(values, weights, capacity):\n",
    "    solution = [] # solution array\n",
    "    obj_val = 0 # accumulated objective\n",
    "    total_weight = 0 # accumulated weight\n",
    "    \n",
    "    for i, weight in enumerate(weights): \n",
    "        if total_weight + weight <= capacity: ## if the item still fits..\n",
    "            solution.append(i) ## add it and \n",
    "            total_weight+= weight # update the accumulated weight\n",
    "            obj_val += values[i] # as well as the optimal objective value\n",
    "    return obj_val, solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..let us try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, [0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = [3000,2000,1500]\n",
    "weights = [30,20,15]\n",
    "capacity = 35\n",
    "\n",
    "greedy_knapsack(values, weights, capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let us try larger instances\n",
    "\n",
    "..there are many instance sets for the 0/1 KP\n",
    "- as an example, there are some instances from D. Pisinger, see the instances folder in the repository associated with this notebook\n",
    "- on the following website, you will find optimal objective function values:\n",
    "\n",
    "http://artemisa.unicauca.edu.co/~johnyortega/instances_01_KP/\n",
    "\n",
    ".. you find some instances in the GitHub repository in which this notebook resides\n",
    "- if you download the zip with this notebook (or clone the repository), you will have them in the folder `problems/knapsack/instances`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading in the instances\n",
    "\n",
    "The following function reads an instance file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def read_knapsack_instance(filename):\n",
    "    weights=[]\n",
    "    values=[]\n",
    "    with open(filename) as f: # open the file\n",
    "        line = f.readline().split()  # split first row\n",
    "        number_of_items = int(line[0]) # read number of items\n",
    "        capacity = int(line[1]) # read capacity\n",
    "        for i in range(number_of_items): # read rows for the items\n",
    "            line = f.readline().split() # split row\n",
    "            values.append(int(line[0])) # read value\n",
    "            weights.append(int(line[1])) # read weight\n",
    "    return np.array(values), np.array(weights), capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... let us try with a 5000-item instance and solve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33727"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"./../problems/knapsack/instances/knapPI_1_5000_1000_1\" # optimal value: 276457 \n",
    "values, weights, capacity  = read_knapsack_instance(filename)\n",
    "\n",
    "obj_value, _ = greedy_knapsack(values, weights, capacity)\n",
    "obj_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Improving the greedy approach by sorting items\n",
    "\n",
    "- one way to improve the performance of this greedy algorithm for the knapsack problem is to sort items\n",
    "- which sorting criteria do you consider promising?\n",
    "- sort the items accordingly and try applying the greedy algorithm to the sorted items\n",
    "\n",
    "#### Hint:\n",
    "In numpy, there is the function `argsort` which does not return the sorted values of an array, but an array of the sorted sorted indexes!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276379"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sorted_indexes = np.argsort(-1* values/weights)\n",
    "\n",
    "sorted_values = values[sorted_indexes]\n",
    "sorted_weights = weights[sorted_indexes]\n",
    "\n",
    "obj_value, _ = greedy_knapsack(sorted_values, sorted_weights, capacity)\n",
    "obj_value\n",
    "\n",
    "\n",
    "#kp_instance = KPInstance(sorted_values, sorted_weights, capacity, len(sorted_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "distance_matrix = np.array([\n",
    "    [0,  5, 4, 10],\n",
    "    [5,  0, 8,  5],\n",
    "    [4,  8, 0,  3],\n",
    "    [10, 5, 3,  0]\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Python library `python-tsp`\n",
    "\n",
    "see: https://github.com/fillipe-gsm/python-tsp\n",
    "\n",
    "### offers:\n",
    "- functions to read TSP instances in the tsplib-format\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from python_tsp.distances import tsplib_distance_matrix\n",
    "\n",
    "#tsplib_file = \"./../problems/tsp/instances/a280.tsp\" # optimal solution 2579 (lt. http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/STSP.html)\n",
    "tsplib_file = \"./../problems/tsp/instances/brazil58.tsp\" # optimal solution 25395 (lt. http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/STSP.html)\n",
    "#tsplib_file = \"./../problems/tsp/instances/berlin52.tsp\" # optimal solution  7542 (lt. http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/STSP.html)\n",
    "\n",
    "distance_matrix = tsplib_distance_matrix(tsplib_file)\n",
    "\n",
    "#permutation, distance = tsp_nearest_neighbor(distance_matrix, [0])\n",
    "#distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- and heuristic as well as exact TSP algorithms\n",
    "  - e.g. local search, simulated annealing and dynamic programming (exact: careful, may take very long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26640"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from python_tsp.heuristics import solve_tsp_local_search, solve_tsp_simulated_annealing\n",
    "\n",
    "#permutation, distance = solve_tsp_local_search(distance_matrix)\n",
    "\n",
    "permutation, distance = solve_tsp_simulated_annealing(distance_matrix)\n",
    "distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A generic wrapper function for calling a function solving the TSP\n",
    "- to simplify our further experiments let us write a function that can call any function solving a TSP\n",
    "- we use `*args` to allow passing arguments (e.g. start node) to the function\n",
    "- the wrapper function\n",
    "  - reads the distance matrix\n",
    "  - solves the problem\n",
    "  - checks the solution for feasibility\n",
    "  - prints instance name, function name, total distance and time spent for solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "instance_name = \"brazil58\"\n",
    "def solve_tsp_using_function(instance_name, tsp_function, *args):\n",
    "    tsplib_file = f\"./../problems/tsp/instances/{instance_name}.tsp\" \n",
    "    distance_matrix = tsplib_distance_matrix(tsplib_file)\n",
    "    starttime = timer()  \n",
    "    permutation, distance = tsp_function(distance_matrix, *args)\n",
    "    if set(permutation) != set(range(len(distance_matrix))):\n",
    "        print (\"Not a proper permutation!\")\n",
    "    \n",
    "    print(f\"{instance_name}, {tsp_function.__name__}, distance: {distance}, time: {timer()- starttime:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trying out our generic wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "..with a function from the TSPLib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brazil58, solve_tsp_simulated_annealing, distance: 25757, time: 2.093\n"
     ]
    }
   ],
   "source": [
    "solve_tsp_using_function(instance_name, solve_tsp_simulated_annealing)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "..with our nearest neighbor function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modeling a discrete multi-stage transition system\n",
    "\n",
    "<img src=\"./img/deterministic_multistage_problem.png\" width=\"60%\">\n",
    "\n",
    "\n",
    "\n",
    "- $k$ the current step / stage (e.g. the number of cities visited so far), out of $N$ stages.\n",
    "- $x_k$ the current state needed to calculate the next step and the cost\n",
    "    - e.g. the cities visited visited so far and the current city\n",
    "    - the start state is defined as $x_0$\n",
    "- $u_k$ a decision from the set $U_k(x_k)$ of feasible decisions when being in stage $k$ and in state $x_k$ \n",
    "  - e.g. a city that was not visited so far  \n",
    "- $g(x_k, u_k)$ the cost of choosing decision $u_k$ when being in state $x_k$\n",
    "  - e.g. the distance to the next city\n",
    "- $f(x_k, u_k)$ a transition function that computes $x_{k+1}$ from $x_k$ und the decision $u_k$ \n",
    "  - e.g. an augmentation of the cities visited so far and an update of the current city\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A dynamic programming model\n",
    "\n",
    "A model for such a discrete system as defined on the previous slide along with the optimization problem:\n",
    "\n",
    "$$\\min_{u_0,..,u_k,..u_{N-1}} \\sum_{k=0}^{N-1} g_k(x_k,u_k)$$\n",
    "\n",
    "\n",
    "..will be referred to as a **dynamic programming model** in this remainder of this course, and we will refer to this generic problem as $DP$ in what follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Observe:\n",
    "- here, we assume a minimization problem, but it is straightforward to obtain a corresponding maximization problem\n",
    "- we also assume the cost are \"stage-wise\"-additive (but: they can be state-dependent!)\n",
    "- we assume there are no terminal costs $g(x_N)$ (would be straightforward to include)\n",
    "- there can be far more general DP models, but for now we stick to classes that can be represented as displayed above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representing a DP model in Python\n",
    "\n",
    "Let us devise a Python framework for implementing a dynamic programming model.\n",
    "\n",
    "To implement a DP model for a CO problem in this framework, we need to implement the following functions:\n",
    "- a function returning the feasible decisions $U_k$ given a state $x_k$ in stage $k$ (and the data from the instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def feasible_decisions(instance, k, state):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the transition function $f(x_k, u_k)$ returning the state $x_{k+1}$ resulting from taking decision $u_k$ when being in state $x_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def transition_function(instance, k, state, decision):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def cost_function(instance, k, state, decision):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representing a DP model in Python\n",
    "\n",
    "- we can collect these three functions in a `NamedTuple` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DP(NamedTuple):\n",
    "    feasible_decisions : Callable\n",
    "    transition_function : Callable\n",
    "    cost_function : Callable\n",
    "    direction : str # 'max' or 'min'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### States\n",
    "- we are free to define our state representation\n",
    "- for later purposes, it will be useful if the state variable is immutable, therefore tuples or namedtuple are useful data structures for states\n",
    "\n",
    "\n",
    "#### Decisions\n",
    "- in most cases in this part, we will assume that decisions are integers, but note that this is not required as long as the transition function works\n",
    "- however, for now, we assume that decisions only induce a change between stages -- we will relax that requirement later in the course\n",
    "\n",
    "\n",
    "#### Instance data\n",
    "- all functions named above take an instance as parameter. Instance data does not have to take a certain form, it just needs to \"match\" the (problem-specific) functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generic helper functions to deal with maximization and minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "@njit \n",
    "def better(value1, value2, direction):\n",
    "    if direction == \"min\":\n",
    "        return value1 < value2\n",
    "    else:\n",
    "        return value1 > value2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def best_element_and_value(elements, values, direction):\n",
    "    if direction == \"min\":\n",
    "        best_index = np.argmin(values)\n",
    "    else:\n",
    "        best_index = np.argmax(values)\n",
    "    return elements[best_index], values[best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def get_n_best_elements_and_values(n, elements, values, direction):\n",
    "   \n",
    "    if direction == \"min\":\n",
    "        sorted_indexes = np.argsort(values)\n",
    "    else:\n",
    "        sorted_indexes = np.argsort(-values)\n",
    "    return elements[sorted_indexes[:n]], values[sorted_indexes[:n]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3, 2]), array([5, 3]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_best_elements_and_values(2, np.array([1,2,3]), np.array([2,3,5]), 'max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Example: A DP model for the Knapsack Problem\n",
    "- given a  knapsack instance with $N$ items with weights $w_k$ and profits $p_k$ (zero-indexed) and capacity $W$ \n",
    "\n",
    "- state $x_k$: accumulated weight after adding the first $k-1$ items, $x_0 = 0$\n",
    "- decision $u_k \\in \\{0, 1\\}$ (0: do not add item $k$ to the knapsack; 1: add item $k$)\n",
    "- $U_k(x_k) = \\begin{cases} \n",
    "                \\{0,1\\} \\quad \\mathrm{if} \\quad x_k + w_k \\leq W \\\\\n",
    "                \\{0 \\} \\quad \\mathrm{else}\n",
    "\\end{cases}$\n",
    "\n",
    "- $f(x_k, u_k) = x_k + w_k u_k $\n",
    "\n",
    "- $g(x_k, u_k) = p_k u_k$\n",
    "\n",
    "We have a maximization-objective:\n",
    "\n",
    "$$\\max_{u_0,..,u_k,..u_{N-1}} \\sum_{k=0}^{N-1} g_k(x_k,u_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Knapsack DP Model in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class  KPInstance(NamedTuple):\n",
    "    values:np.array\n",
    "    weights:np.array\n",
    "    capacity:int\n",
    "    N:int   \n",
    "\n",
    "@njit\n",
    "def feasible_decisions_kp(instance, k, acc_weight):    \n",
    "    if acc_weight + instance.weights[k] <= instance.capacity: return [0,1]  \n",
    "    else: return [0]      \n",
    "\n",
    "@njit\n",
    "def transition_function_kp(instance, k, acc_weight, put):\n",
    "    return acc_weight + put*instance.weights[k]\n",
    "\n",
    "@njit\n",
    "def cost_function_kp(instance, k, acc_weight, put):\n",
    "      return put*instance.values[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Putting all together, and stating that we have a maximization objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dp_kp = DP(feasible_decisions_kp, transition_function_kp,  cost_function_kp, \"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An instance reader function for the Knapsack Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_kp_instance(filename, sorted=True):\n",
    "    weights=[]\n",
    "    values=[]\n",
    "    with open(filename) as f: # open the file\n",
    "        line = f.readline().split()  # split first row\n",
    "        number_of_items = int(line[0]) # read number of items\n",
    "        capacity = int(line[1]) # read capacity\n",
    "        for i in range(number_of_items): # read rows for the items\n",
    "            line = f.readline().split() # split row\n",
    "            values.append(int(line[0])) # read value\n",
    "            weights.append(int(line[1])) # read weight\n",
    "            \n",
    "    values = np.array(values)\n",
    "    weights = np.array(weights)    \n",
    "    if sorted:\n",
    "        sorted_indexes = np.argsort(-1* values/weights)\n",
    "    values = values[sorted_indexes]\n",
    "    weights = weights[sorted_indexes]\n",
    "     \n",
    "        \n",
    "    return KPInstance(values, weights, capacity, number_of_items)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"./../problems/knapsack/instances/knapPI_1_5000_1000_1\"\n",
    "filename = \"./../problems/knapsack/instances/knapPI_1_100_1000_1\"\n",
    "kp_instance = read_kp_instance(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: A DP model for the TSP\n",
    "\n",
    "Given:\n",
    "- a TSP instance with a $N$ cities and distances $d_{i,j}$ between cities $i,j$\n",
    "  - let us denote with $\\mathcal{N} = \\{1, \\ldots N \\}$ the set of cities \n",
    "  \n",
    "\n",
    "\n",
    "- state $x_k$: sequence / ordered set of cities visited so far, $x_0 = i^0$ where $i^0$ is the first city\n",
    "  - let us define $l(x_k)$ as the last element in the ordered set, that is, the \"current\" city\n",
    "\n",
    "- decision $u_k \\in \\mathcal{N}$ city to visit next \n",
    "- $U_k(x_k) = \\mathcal{N} \\setminus x_k$\n",
    "- $f(x_k, u_k) = x_k + u_k$  (here, with $+$ we mean to append $u_k$ to the sequence / ordered set $x_k$\n",
    "- $g(x_k, u_k) = \\begin{cases} \n",
    "                d_{l(x_k), u_k} \\quad \\mathrm{if} \\quad k < N-1 \\\\\n",
    "               d_{l(x_k), u_k} +  d_{u_k, i^0} \\quad  \\mathrm{if} \\quad k = N-1\n",
    "\\end{cases}$\n",
    "\n",
    "We have a minimization objective:\n",
    "\n",
    "$$\\min_{u_0,..,u_k,..u_{N-1}} \\sum_{k=0}^{N-1} g_k(x_k,u_k)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: DP model for the TSP in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class TSPInstance(NamedTuple):\n",
    "    distance_matrix : np.array\n",
    "    N : int\n",
    "\n",
    "@njit\n",
    "def feasible_decisions_tsp(instance, k, sequence):\n",
    "    return np.array([i for i in range(instance.N) if i not in sequence ])    \n",
    "\n",
    "@njit\n",
    "def transition_function_tsp(instance, k, sequence, neighbor):\n",
    "    return sequence + [neighbor]\n",
    "\n",
    "@njit\n",
    "def cost_function_tsp(instance, k, sequence, neighbor):\n",
    "    \n",
    "    if k < instance.N-1:\n",
    "        return instance.distance_matrix[sequence[k-1]][neighbor]\n",
    "    else:\n",
    "        return instance.distance_matrix[sequence[k-1]][neighbor] + instance.distance_matrix[neighbor][sequence[0]]\n",
    "\n",
    "dp_tsp = DP(feasible_decisions_tsp, transition_function_tsp,  cost_function_tsp, \"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An instance reader function for the TSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsp_instance(filename):\n",
    "    \n",
    "    distance_matrix = tsplib_distance_matrix(filename)\n",
    "    return TSPInstance(distance_matrix, len(distance_matrix))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplib_file = \"./../problems/tsp/instances/brazil58.tsp\" # optimal solution 25395 (lt. http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/STSP.html)\n",
    "#tsplib_file = \"./../problems/tsp/instances/berlin52.tsp\" # optimal solution  7542 (lt. http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/STSP.html)\n",
    "\n",
    "tsp_instance = read_tsp_instance(tsplib_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generic implentations of algorithms based on the generic DP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generic myopic greedy for DP models\n",
    "\n",
    "- given a generic DP model, we can now start devising generic implementation of algorithms operating on DP models\n",
    "- as an example, we can generically implement greedy as follows:\n",
    "\n",
    "**Observe:**\n",
    "- below, we avoid some loops by directly working on arrays of decisions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def dp_greedy(dp, instance, k_start, state_start):\n",
    "    \n",
    "    state = state_start\n",
    "    \n",
    "    total_cost = 0\n",
    "    \n",
    "    for k in range(k_start, instance.N):    \n",
    "        \n",
    "        # get decisions and associated costs\n",
    "        decisions = dp.feasible_decisions(instance, k, state)               \n",
    "        costs = np.array([dp.cost_function(instance, k, state, d) for d in decisions])\n",
    "        \n",
    "        #get the best decision according to one-step costs\n",
    "        best_decision, best_cost = best_element_and_value(decisions, costs, dp.direction) \n",
    "        \n",
    "        state = dp.transition_function(instance, k, state, best_decision)\n",
    "\n",
    "        total_cost += best_cost\n",
    "        \n",
    "    return  state, total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Try with both KP and TSP:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Knapsack Problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8817"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_greedy(dp_kp, kp_instance, 0, 0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "TSP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30774"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_greedy(dp_tsp, tsp_instance, 1, [0])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Greedy as a myopic policy \n",
    "\n",
    "- we will see later how to solve a $DP$ model to optimality\n",
    "\n",
    "- in general, we refer to a function $\\pi$ that maps a state $x_k$ to a decision $u_k$ as a **policy**\n",
    "- in a deterministic problem, given a policy $\\pi$, we can obtain a solution to $DP$ by \n",
    "  - starting from $x_k := x_0$ and selecting the $u_k$ according to the policy\n",
    "  - applying the state transition $x_{k+1} = f(x_k, u_k)$\n",
    "  - and continue until $k:= N -1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Greedy as a policy:\n",
    "- we can view the greedy algorithm as being based on a policy that selects a $u_k$ that minimizes the transition costs $g$:\n",
    "\n",
    "$$u_k = \\underset{u_k \\in U_k(x_k)}{\\operatorname{argmin}} \\, g(x_k, u_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Observe: This policy is **myopic** since it does not account for how deciding for a certain $u_k$ affects the quality of the remaining solution process - hence its name: *greedy*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accounting for the future: The value function \n",
    "\n",
    "\n",
    "- to quantify the future value (also called cost-to-go) of a state $x_k$, we use the so-called value function $J(x_k)$ \n",
    "  - given a state $x_k$, $J(x_k)$ represents the cost / value obtained by solving the residual problem from stages $k$ to $N-1$.\n",
    "  - the corresponding problem starting at $k$ is also referred to as the **tail subproblem**.\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " \n",
    "   \n",
    "Given a value function, we can compute the decision to take in stage  $k$ as:\n",
    "\n",
    "$$u_k = \\underset{u_k \\in U_k(x_k)}{\\operatorname{argmin}} \\, \\Big( g(x_k, u_k) + J(f(x_k, u_k)) \\Big) $$\n",
    "\n",
    "\n",
    "Observe that for the greedy policy for the knapsack, $J(x_{k}) = 0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Q-values / Q-factors\n",
    "\n",
    "- in some cases, in particular in some reinforcement learning approaches, it is convenient to use so-called Q-factors,  Q-values or Q-functions\n",
    "\n",
    "$$Q_k(x_k, u_k) = g(x_k, u_k) + J\\big( f(x_k, u_k ) \\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "..using these Q-factors, we can re-write the problem of selecting the next decision / control / action as:\n",
    "\n",
    "$$u_k = \\underset{u_k \\in U_k(x_k)}{\\operatorname{argmin}} \\, Q_k(x_k, u_k) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimal and approximate value functions\n",
    "\n",
    "\n",
    "#### The optimal (exact) value function\n",
    "- we denote the exact / optimal value function with $J^*$.\n",
    "- if we have access to $J^*$, then the greedy policy based on $g(x_k, u_k) + J(f(x_k, u_k))$ gives us an optimal solution\n",
    "- the problem: $J^*$ is typically probitively hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Approximate value functions\n",
    "\n",
    "- we denote an approximate value function with $\\tilde{J}$\n",
    "- a greedy policy based on $\\tilde{J}$ is suboptimal,  but can be much faster to compute\n",
    "- approximate value functions can be determined in various ways $\\tilde{J}$:\n",
    "  - using offline training / learning\n",
    "  - using problem simplification or aggregation (solve an approximate tail problem)\n",
    "  - using online techniques (e.g. rollout), see later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generic Rollout with a Base Heuristic\n",
    "\n",
    "- recall that given an approximate value function $\\tilde{J}$, we can construct a policy that takes the decision according to the best approximate $Q-$-value $\\tilde{Q}_k(x_k, u_k)$\n",
    "\n",
    "$$u_k = \\underset{u_k \\in U_k(x_k)}{\\operatorname{argmin}} \\, \\tilde{Q}_k(x_k, u_k) = \\underset{u_k \\in U_k(x_k)}{\\operatorname{argmin}} \\, \\Big( g(x_k, u_k) +  \\tilde{J}(f(x_k, u_k)) \\Big) $$\n",
    "  \n",
    "  \n",
    "- key idea of rollout:  run a (simple and fast) base heuristic on the tail subproblem starting from $x_{k+1} = f(x_k, u_k)$ to obtain a cost / value $H(f(x_k, u_k))$, and use that value as value function approximation:\n",
    "  - $\\tilde{J}(x_{k+1}) = H (x_{k+1})$\n",
    "\n",
    "\n",
    "<img src=\"./img/rollout_general.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Generic rollout with greedy base heuristic in Python\n",
    "- given that we have a generic greedy, we can also implement a generic rollout algorithm\n",
    "- observe: by using `@njit(parallel=True)`, numba will try parallelzing the list comprehension involving the dp_greedy call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def dp_rollout(dp, instance, k_start, state_start):\n",
    "    \n",
    "    state = state_start\n",
    "    \n",
    "    total_cost = 0\n",
    "    \n",
    "    for k in range(k_start, instance.N):    \n",
    "         \n",
    "        decisions = dp.feasible_decisions(instance, k, state)        \n",
    "        costs = np.array([dp.cost_function(instance, k, state, d) for d in decisions])        \n",
    "    \n",
    "        next_states = [dp.transition_function(instance, k, state, d) for d in decisions]\n",
    "        \n",
    "        # here, we now rund the generic greedy\n",
    "        approx_costs_to_go = np.array([dp_greedy(dp, instance, k+1, next_state)[1] for next_state in next_states])\n",
    "        \n",
    "        # now, decision is made based on one-step cost + cost-to-go-approximation\n",
    "        best_decision,_ = best_element_and_value(decisions, costs+approx_costs_to_go, dp.direction)\n",
    "        \n",
    "        state = dp.transition_function(instance, k, state, best_decision)\n",
    "\n",
    "        total_cost += dp.cost_function(instance, k, state, best_decision)\n",
    "        \n",
    "    return state, total_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trying it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 781 ms\n",
      "Wall time: 608 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8929"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dp_rollout(dp_kp, kp_instance,0,0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 141 ms\n",
      "Wall time: 138 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28131"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dp_rollout(dp_tsp, tsp_instance,1,[0])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generic simplified rollout\n",
    "\n",
    "- recall that, just as Bertsekas in his books, use the term `simplified` to state that only a promising subset of all decisions in a given stage will be evaluated using the greedy heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def dp_simplified_rollout(dp, instance, k_start, state_start, n_candidates):\n",
    "    \n",
    "    state = state_start\n",
    "    \n",
    "    total_cost = 0\n",
    "    \n",
    "    for k in range(k_start, instance.N):    \n",
    "        \n",
    "        \n",
    "        decisions = np.array(dp.feasible_decisions(instance, k, state))\n",
    "        \n",
    "        costs = np.array([dp.cost_function(instance, k, state, d) for d in decisions])\n",
    "        \n",
    "        \n",
    "        # simplification step!\n",
    "        decisions, costs = get_n_best_elements_and_values(n_candidates, decisions, costs, dp.direction)\n",
    "        \n",
    "        \n",
    "        next_states = [dp.transition_function(instance, k, state, d) for d in decisions]\n",
    "        \n",
    "        approx_costs_to_go = np.array([dp_greedy(dp, instance, k+1, next_state)[1] for next_state in next_states])\n",
    "        \n",
    "        best_decision,_ = best_element_and_value(decisions, costs+approx_costs_to_go, dp.direction)\n",
    "        \n",
    "        state = dp.transition_function(instance, k, state, best_decision)\n",
    "\n",
    "        total_cost += dp.cost_function(instance, k, state, best_decision)\n",
    "        \n",
    "    return state, total_cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trying it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 828 ms\n",
      "Wall time: 759 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8929"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dp_simplified_rollout(dp_kp, kp_instance, 0, 0, 10)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dp_simplified_rollout(dp_tsp, tsp_instance, 1, [0], 10)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-step lookahead\n",
    "\n",
    "- in (exact) dynamic programming (by reaching), we (somewhat) construct a full state-transition graph\n",
    "- in rollout, in each iteration, only the first step is \"exact\", the rest of the graph is approximated\n",
    "- in multi-step lookahead, we partially expand the tree for (more than one stage) to have more \"exact\" steps before using a value function approximation for selection\n",
    "- below: multi-step lookahead with rollout for value function approximation\n",
    "\n",
    "<img src=\"./img/multistep_lookahead.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generic Simplified Multistage Lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def dp_simplified_multi_stage_lookahead_rollout(dp, instance, k_start, state_start, n_candidates, number_of_lookahead_steps = 1):\n",
    "    \n",
    "    state = state_start\n",
    "    \n",
    "    total_cost = 0\n",
    "    \n",
    "    for k in range(k_start, instance.N):    \n",
    "        \n",
    "        \n",
    "        decisions = np.array(dp.feasible_decisions(instance, k, state))\n",
    "        \n",
    "        costs = np.array([dp.cost_function(instance, k, state, d) for d in decisions])\n",
    "        \n",
    "        # simplification step\n",
    "        decisions, costs = get_n_best_elements_and_values(n_candidates, decisions, costs, dp.direction)\n",
    "        \n",
    "        next_states = [dp.transition_function(instance, k, state, d) for d in decisions]\n",
    "                \n",
    "        if number_of_lookahead_steps == 1: #this is basically rollout\n",
    "            \n",
    "            approx_costs_to_go = np.array([dp_greedy(dp, instance, k+1, next_state)[1] for next_state in next_states])\n",
    "        else:\n",
    "            approx_costs_to_go = np.array([dp_simplified_multi_stage_lookahead_rollout(dp, instance, k+1, next_state, n_candidates, number_of_lookahead_steps - 1)[1] for next_state in next_states])\n",
    "            \n",
    "            \n",
    "        best_decision,_ = best_element_and_value(decisions, costs+approx_costs_to_go, dp.direction)\n",
    "        \n",
    "        state = dp.transition_function(instance, k, state, best_decision)\n",
    "\n",
    "        total_cost += dp.cost_function(instance, k, state, best_decision)\n",
    "        \n",
    "    return state, total_cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trying it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Task: Scheduling jobs on two identical machines with time-dependent processing times, minimizing total completion time\n",
    "\n",
    "Let us consider the following problem:\n",
    "- we are given a set $J$ of of jobs that can be processed on two identical machines, starting at period $t$ = 1\n",
    "- each job has a (machine-independent) \"basic\" processing time of $b_j$; however, the processing time depends on the start time $t_j$ at which job $j$ is started: the processing time is then $b_jt_j$\n",
    "- jobs cannot be interrupted\n",
    "- we are looking for an assignment of jobs to machines such that the total completion time, that is, the sum of the total processing time on both machines is minimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- as a toy instance, consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "basic_job_durations = [4,7,9,3,8,5,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- for large instances, just use the knapsack instances, using the items as jobs and the weights as basic processing times, or randomly generate instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Task:** Write a DP model (both \"on paper\" and in Python) for that problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solving exactly with DP by Reaching\n",
    "\n",
    "- in all the approaches discussed so far, we constructed the solution stage-by-stage, in a single pass\n",
    "- one possible exact approach is to \n",
    "\n",
    "- we can call the resulting graph the state-transition graph of the DP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exact Dynamic Programming: An Illustration of the Knapsack Case\n",
    "\n",
    "One approach to exactly solve a DP model is to\n",
    "- create the state transition graph and\n",
    "- compute the shortest (longest) path in the graph\n",
    "\n",
    "\n",
    "<img src=\"./img/reaching_05.png\" width=\"60%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Diagrams for Optimization: An Overview\n",
    "\n",
    "An **exact** Decision Diagram is (almost) the same as the state space graph of a DP for a maximization problem\n",
    "\n",
    "However, the field of DDs for optimization involve a set of interesting and generic concepts:\n",
    "- an exact reduction scheme that allows reducing the size of an exact DD\n",
    "- restricted DDs for compactly representing a subset of all feasible solutions and for obtaining lower bounds\n",
    "- relaxed DDs compactly representing a superset of all feasible solutions and for obtaining upper bounds\n",
    "- a generic branch-and-bound scheme only relying on restricted and relaxed DDs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exact Decision Diagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- given a DP model, we can view an exact DD as a state-transition-graph, with one exception:\n",
    "  - we introduce a terminal node that forms the target of all arcs emanating from layer $N-1$\n",
    "- just as in the DP by reaching algorithm, we can construct the exact DD by \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Decision Diagram data structure\n",
    "\n",
    "We will introduce a class `DecisionDiagram` that represents a DD\n",
    "- consisting of $N$ + 1 layers indexed from 0 to $N$\n",
    "    - each layer is a dictionary where the key is a state and the value is a `NodeInfo` object\n",
    "  - (problem-specific) state values representing the start (source) state and the sink state\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class DecisionDiagram:        \n",
    "    def __init__(self, number_of_layers, source_layer, source_state, sink_state, direction = 'max'):    \n",
    "        self.number_of_layers = number_of_layers\n",
    "        self.layers = [dict() for l in range(0, number_of_layers)]\n",
    "        self.source_state = source_state\n",
    "        self.sink_state = sink_state\n",
    "        self.layers[source_layer][source_state] = NodeInfo()\n",
    "        self.direction = direction\n",
    "        self.last_exact_layer = source_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Storing node information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A `NodeInfo` object contains\n",
    "  - `decisions_succ_states`: a `dict` with key `decision` and value: state (the 'out-arcs' of the node)\n",
    "  - `pred_state_decisions`: a `set` of (state, decision)-tuples (the 'in-arcs') of the node\n",
    "  - `best_dist`: the best distance from source to the state found so far \n",
    "  - `best_pred_state_decision`: the tuple representing the predecessor state and decision leading to the best distance\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass # observe: we use dataclass here instead of tuple because we need mutability\n",
    "class NodeInfo():\n",
    "    decisions_succ_states : dict = field(default_factory=dict)\n",
    "    pred_state_decisions: set = field(default_factory=set)\n",
    "    best_dist : int = 0\n",
    "    best_pred_state_decision: tuple = None\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeInfo(decisions_succ_states=0, pred_state_decisions=set(), best_dist=0, best_pred_state_decision=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NodeInfo(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building an exact DD by top-down-compilation\n",
    "- building an exact DD is basically the same as building the DP by reaching: states are \"discovered\" layer per layer\n",
    "- by applying the transition function to each feasible decision in each state in the layer under consideration\n",
    "- in the following algorithm, we store the best distance from the source / root node as well as the preceding node in each node\n",
    "- this means that the best path in the DD is computed \"in passing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building an exact DD by top-down-compilation in Python\n",
    "- observe: here, we introduce a sink state as a \"dummy\" state (that is otherwise not reachable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_exact_dd(dp, instance, start_layer, start_state, sink_state):\n",
    "    \n",
    "    dd = DecisionDiagram(instance.N+1, start_layer, start_state, sink_state, dp.direction)\n",
    "    \n",
    "    state = start_state\n",
    "    total_cost = 0\n",
    "    \n",
    "    for k in range(0,instance.N):\n",
    "        \n",
    "        for state, node_info in dd.layers[k].items():\n",
    "            decisions = dp.feasible_decisions(instance, k, state)\n",
    "            \n",
    "            for decision in decisions:\n",
    "                if k < instance.N -1: # if we are the final layer, point to the \"sink state\"\n",
    "                    next_state = dp.transition_function(instance,k,state, decision)\n",
    "                else:\n",
    "                    next_state = -1\n",
    "                \n",
    "                add_transition_dd(dd, k, state, decision, next_state, dp.cost_function(instance, k, state, decision))\n",
    "   \n",
    "    k = instance.N-1\n",
    "    \n",
    "    return dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating new nodes: adding the result of a transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transition_dd(dd, layer_index, state, decision, result_state, cost):\n",
    "    \n",
    "    layer = dd.layers[layer_index]\n",
    "    result_layer = dd.layers[layer_index+1] \n",
    "    \n",
    " \n",
    "    result_dist = layer[state].best_dist + cost  \n",
    "\n",
    "    node_info = result_layer.get(result_state) \n",
    "\n",
    "    # add new node if state does not exist in that layer\n",
    "    if node_info is None: \n",
    "        node_info = NodeInfo()\n",
    "        node_info.best_dist = result_dist\n",
    "        node_info.best_pred_state_decision = (state, decision)\n",
    "        result_layer[result_state] = node_info            \n",
    "    \n",
    "    else:  ## check if betterpath \n",
    "        if better(result_dist, node_info.best_dist, dd.direction): \n",
    "            node_info.best_pred_state_decision = (state, decision)\n",
    "            node_info.best_dist = result_dist\n",
    "\n",
    "    node_info.pred_state_decisions.add((state, decision))\n",
    "\n",
    "    layer[state].decisions_succ_states[decision] = result_state\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trying it out, and some utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dd = build_exact_dd(dp_kp, kp_instance, 0, 0,-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "..getting the best objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_best_objective (dd):\n",
    "    return dd.layers[dd.number_of_layers-1][dd.sink_state].best_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9147"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_objective (dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "..getting the best path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_best_path(dd):\n",
    "    \n",
    "    decisions = []\n",
    "    \n",
    "    state =  dd.sink_state\n",
    "    k = dd.number_of_layers - 2\n",
    "        \n",
    "    while k >= 0:\n",
    "       # print(k)\n",
    "        state, decision = dd.layers[k+1][state].best_pred_state_decision\n",
    "        #print (dd.layers[k+1])\n",
    "        decisions.append(decision)\n",
    "        k = k-1\n",
    "        \n",
    "\n",
    "    return dd.layers[dd.number_of_layers-1][-1].best_dist, list(reversed(decisions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "get_best_path(dd)[1][:10] ## first 10 nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "..getting the number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_number_of_nodes(dd):\n",
    "    return np.sum([len(layer) for layer in dd.layers])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_number_of_nodes(dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reducing an exact DD\n",
    "\n",
    "One of the key ideas from DDs is that very often, a DD can be compressed / reduced by merging nodes \n",
    "that \n",
    "- do not have identical (top-down) states\n",
    "- but are nonetheless **equivalent** in the sense that they have the same *completions*, that is, the same set of partial solutions until the end (the solution sets of their tail subproblems are identical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This type of equivalence can be identified by an upward-pass starting from the bottom layer $N$ to layer $0$\n",
    "\n",
    "- in each layer $k$, two nodes are equivalent (are in the same equivalence class) if \n",
    "  - they have the same set of feasible decisions \n",
    "  - these decisions have the same costs\n",
    "  - the corresponding arcs point to the same set of nodes in the subsequent layer $k+1$\n",
    "- for each equivalence class, merge all nodes in that class into a single node\n",
    "\n",
    "**Attention:** The following implementation assumes that the decision costs are state-independent. If the decision costs (the arc costs) are state-dependent, then we need to add a check for identical costs, too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementing the DD reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_exact_dd(dd):\n",
    "    \n",
    "    #proceed from the bottom (last layer) to the top\n",
    "    k = len(dd.layers)-1\n",
    "    while k > 0:\n",
    "        \n",
    "        # a dict with key: decisions and resulting nodes (forming an equivalence class)\n",
    "        #       and value: list of states falling into that class\n",
    "        eq_classes = {}\n",
    "    \n",
    "        #1. collect equivalence classes and states/nodes in each class\n",
    "        for state, node_info in dd.layers[k].items():\n",
    "    \n",
    "            eq_class = tuple(node_info.decisions_succ_states)            \n",
    "            if eq_class not in eq_classes:\n",
    "                eq_classes[eq_class] = [state]                \n",
    "            else:\n",
    "                eq_classes[eq_class].append(state)\n",
    "        \n",
    "        # 2. merge all states in each class into a single node\n",
    "        for eq_class, states in eq_classes.items():            \n",
    "            while len(states) > 1:\n",
    "                state_remove = states.pop()\n",
    "                merge_nodes(dd, k, states[0], state_remove)\n",
    "        k=k-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Merging two nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def merge_nodes(dd, layer_index, state_orig, state_remove):\n",
    "    layer = dd.layers[layer_index]\n",
    "    next_layer = dd.layers[layer_index+1]\n",
    "    prev_layer = dd.layers[layer_index-1]\n",
    "\n",
    "    # 1. Keep the best distance to from the source\n",
    "    if better(layer[state_remove].best_dist, layer[state_orig].best_dist, dd.direction):\n",
    "            layer[state_orig].best_pred_state_decision = layer[state_remove].best_pred_state_decision\n",
    "            layer[state_orig].best_dist = layer[state_remove].best_dist\n",
    "\n",
    "    # 2. remove the in-arcs of the successors that come from the removed node\n",
    "    for decision, succ_state in layer[state_remove].decisions_succ_states.items():\n",
    "        next_layer[succ_state].pred_state_decisions.remove((state_remove, decision))\n",
    "        if next_layer[succ_state].best_pred_state_decision == (state_remove, decision):\n",
    "            next_layer[succ_state].best_pred_state_decision = (state_orig, decision)\n",
    "\n",
    "    # 3. redirect the in-arcs from the removed node to the node to be kept\n",
    "    for pred_state, decision in layer[state_remove].pred_state_decisions:\n",
    "        prev_layer[pred_state].decisions_succ_states[decision] = state_orig\n",
    "        layer[state_orig].pred_state_decisions.add((pred_state, decision))\n",
    "\n",
    "    # 4. Remove the node\n",
    "    layer.pop(state_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trying it  out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "reduce_exact_dd(dd)\n",
    "\n",
    "print (\"reduced nodes\", get_number_of_nodes(dd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..some sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7276"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_objective (dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_path(dd)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Restricted Decision Diagrams\n",
    "\n",
    "- building an exact decision diagram is often not practical since it may have an exponential size\n",
    "- also, the reduction requires building the exact DD beforehand\n",
    "- the idea of restricted DDs is to limit the size of the DD (more precisely, the width `maxWidth` of its layers) by removing states from each layer until the maximum width is respected\n",
    "- of course, this introduces an approximation, that is, the solution is no longer guaranteed to be optimal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a restricted DD top-down in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_restricted_dd(dp, instance, start_layer, start_state, sink_state, max_width,  sort_key_function=None):\n",
    "    \n",
    "    dd = DecisionDiagram(instance.N+1, start_layer, start_state, sink_state, dp.direction)  \n",
    "    sort_key_function = get_sort_key_function(dp)\n",
    "    \n",
    "    state = start_state\n",
    "    total_cost = 0    \n",
    "    \n",
    "    for k in range(start_layer,instance.N):\n",
    "\n",
    "         for state, node_info in dd.layers[k].items():\n",
    "            decisions = dp.feasible_decisions(instance, k, state)\n",
    "            \n",
    "            for decision in decisions:\n",
    "                if k < instance.N -1:\n",
    "                    next_state = dp.transition_function(instance,k,state, decision)\n",
    "                else:\n",
    "                    next_state = -1\n",
    "                \n",
    "                add_transition_dd(dd, k, state, decision, next_state, dp.cost_function(instance, k, state, decision)) \n",
    "                \n",
    "         # remove_nodes until max_width is reached\n",
    "        remove_until_max_width(dd, k+1,max_width, sort_key_function)       \n",
    "\n",
    "    return dd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Removing all nodes in a layer until maximum width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def remove_until_max_width(dd, layer_index, max_width, sort_key_function):\n",
    "\n",
    "    if len(dd.layers[layer_index]) <= max_width:\n",
    "        if dd.last_exact_layer == layer_index -1: \n",
    "            dd.last_exact_layer = layer_index\n",
    "        return\n",
    "\n",
    "    sorted_nodes = sorted(dd.layers[layer_index].items(), key=sort_key_function)\n",
    "    dd.layers[layer_index] = dict(sorted_nodes[0:max_width])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some standard sort function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_key_max_distance(state_node):\n",
    "    return state_node[1].best_dist * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_key_min_distance(state_node):\n",
    "    return state_node[1].best_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sort_key_function_best_distance(dp):\n",
    "    \n",
    "    if dp.direction == 'max':\n",
    "        return sort_key_max_distance\n",
    "    else:\n",
    "        return sort_key_min_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = build_restricted_dd(dp_kp, kp_instance, 0, 0,-1,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_objective (dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Task: Other sort functions\n",
    "\n",
    "**Try out other sort functions!**\n",
    "- using min distance instead of max distance for the KP#\n",
    "- write your own function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relaxed Decision Diagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- restricted DDs have a **limited size**, represent a **subset** of all feasible solutions of a DP, and thus give us an approximate feasible solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- relaxed DDs have a **limited size**, represent a **superset** of all feasible solutions of a DP, and thus give us an upper (lower) bound in case of a maximization (minimization) problem\n",
    "    - how can that work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    \n",
    "**Key idea:** By merging nodes that are not equivalent!\n",
    "- to obtain a proper relaxation (to make sure that we have a superset of all feasible solutions, we apply a so-called **merge operation** $\\oplus$ that makes sure that \n",
    "    - the solutions (paths to the terminal) starting from the merged state $s' = s_1 \\oplus s_2$  form a superset of the union of the solutions (paths to the terminal) starting from $s_1$ and $s_2$\n",
    "    - and that the best objective value starting from $s_1 \\oplus s_2$ is at least as good as the \n",
    "- in simplified terms, $\\oplus$ is chosen in a way that $s_1 \\oplus s_2$ is less restrictive that both $s_1$ and $s_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Top-down compilation of relaxed DDs: Python implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_relaxed_dd(dp, merge_function, instance, start_layer, start_state, sink_state, max_width, sort_key_function=None):\n",
    "    \n",
    "    dd = DecisionDiagram(instance.N+1, start_layer, start_state, sink_state)\n",
    "    \n",
    "    if sort_key_function is None:\n",
    "        sort_key_function = get_sort_key_function_best_distance(dp)\n",
    "    \n",
    "    state = start_state\n",
    "    total_cost = 0\n",
    "    \n",
    "    \n",
    "    for k in range(start_layer,instance.N):\n",
    "       \n",
    "        for state, node_info in dd.layers[k].items():\n",
    "            decisions = dp.feasible_decisions(instance, k, state)\n",
    "            \n",
    "            for decision in decisions:\n",
    "                if k < instance.N -1:\n",
    "                    next_state = dp.transition_function(instance,k,state, decision)\n",
    "                else:\n",
    "                    next_state = -1\n",
    "                \n",
    "                add_transition_dd(dd, k, state, decision, next_state, dp.cost_function(instance, k, state, decision))\n",
    "        \n",
    "\n",
    "        merge_until_max_width(dd, merge_function, k+1, max_width, sort_key_function)\n",
    "     \n",
    "\n",
    "    return dd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Merge until the maximum is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    " def merge_until_max_width(dd, merge_function, layer_index, max_width, sort_key_function):\n",
    "        \n",
    "        layer = dd.layers[layer_index]\n",
    "        prev_layer = dd.layers[layer_index-1]\n",
    "        \n",
    "        if len(layer) <= max_width:\n",
    "            if dd.last_exact_layer == layer_index -1: \n",
    "                dd.last_exact_layer = layer_index\n",
    "            return\n",
    "        \n",
    "        # 1. sort the nodes, return  a list of tuples\n",
    "        sorted_nodes = sorted(dd.layers[layer_index].items(),key=sort_key_function)\n",
    "        \n",
    "        # 2. turn the tuples into a layer of length max_width - 1 \n",
    "        dd.layers[layer_index] = dict(sorted_nodes[0:max_width-1])\n",
    "        \n",
    "        \n",
    "        # 3. merge the remaining states into a single (relaxed) node\n",
    "        \n",
    "        # begin with the first state\n",
    "        state, node_info = sorted_nodes[max_width-1]\n",
    "        \n",
    "        # 3a: compute the merged state as well as the best distance of the new node        \n",
    "        for state_next, node_info_next in sorted_nodes[max_width:]:            \n",
    "            state = merge_function(state, state_next) # compute the merged state            \n",
    "            if better(node_info_next.best_dist, node_info.best_dist, dd.direction): #compute\n",
    "                node_info.best_pred_state_decision = node_info_next.best_pred_state_decision\n",
    "                node_info.best_dist = node_info_next.best_dist\n",
    "            \n",
    "        # 3b: make sure everything is properly linked to the predecessors\n",
    "        node_info.pred_state_decisions.clear()\n",
    "        \n",
    "        for state_next, node_info_next in sorted_nodes[max_width-1:]:\n",
    "            \n",
    "            for pred_state, decision in node_info_next.pred_state_decisions:\n",
    "                prev_layer[pred_state].decisions_succ_states[decision] = state\n",
    "                node_info.pred_state_decisions.add((pred_state, decision))\n",
    "            \n",
    "           \n",
    "        # 3c: finally, add the merged node to the layer: now the layer should be max_width\n",
    "        dd.layers[layer_index][state] = node_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trying the whole procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dist 7276 nodes 630\n"
     ]
    }
   ],
   "source": [
    "dd = build_relaxed_dd(dp_kp, min, kp_instance, 0, 0, -1, 200)\n",
    "\n",
    "\n",
    "print (\"dist\", get_best_objective(dd), \"nodes\", get_number_of_nodes(dd))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Task: Other sort functions\n",
    "\n",
    "**Try out other sort functions!**\n",
    "- using min distance instead of max distance for the KP#\n",
    "- write your own function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DD-based Branch-and-Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- for many problems, building the full exact DD is not feasible within an acceptable amount of time\n",
    "- limited-size restricted and relaxed DDs (only) provide bounds\n",
    "- however, they can be used in a DD-specific branch-and-bound scheme!\n",
    "- that scheme was used to successfully solve a number of combinatorial optimization problems, in some cases achieving state-of-the-art performance\n",
    "- the scheme is highly parallelizable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DD-based Branch-and-Bound: Key ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Key idea I: Branching on nodes in exact cutsets**\n",
    "\n",
    "- in every relaxed (and restricted) DD, there are some nodes that are exact\n",
    "- then, in a relaxed DD, we can identify so-called **exact cutsets:**\n",
    "  - a **cutset** is a subset of nodes such that all source-terminal paths pass through at least one node in that subset\n",
    "  - a cutset is called **exact** if all nodes represent exact states (not \"relaxed\" by the merge operation)\n",
    "  \n",
    "- now, instead of of increasing the width in layers after the exact cutset, the branch-and-bound starts building a \"new\" relaxed DD for the subproblem starting from each node in the cutset\n",
    "- the nodes from the cutset are considered as \"open nodes\" which are processed one after the other\n",
    "- when creating a \"new\" relaxed DD, we once again obtain an exact cutset that is then added to the set of open nodes\n",
    "\n",
    "..one of the simplest ways to obtain an exact cutset is to use the \"last exact layer\", that is, the last layer in which no nodes needed to be removed / merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Key idea II: Bounding** \n",
    "- before building the relaxed DD for the current node, we build a restricted DD for the subproblem to (hopefully) obtain a new best feasible solution\n",
    "- if the relaxed DD starting from the subproblem yields a solution that is worse than that \"primal\" solution, its exact cutset is not added to the set of open nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DD-based Branch-and-Bound: Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import PriorityQueue\n",
    "\n",
    "\n",
    "def branch_and_bound_dd(dp, merge_function, instance, start_layer, start_state, sink_state, max_width):    \n",
    "    \n",
    "    best_feasible_obj = 0    \n",
    "      \n",
    "    factor_p_queue = 1\n",
    "    if dp.direction == 'min':\n",
    "        factor_p_queue = -1\n",
    "    \n",
    "    # initialise the set of open states (we use a priority queue here)\n",
    "    # we store obj-function * factor as key for determining the priority, state and layer in the queue\n",
    "    open_states = PriorityQueue()\n",
    "    open_states.put((0, start_state, 0))\n",
    "    \n",
    "    number_of_open_states_considered = 1\n",
    "    \n",
    "    while open_states.qsize() > 0:\n",
    "        number_of_open_states_considered += 1\n",
    "        dist_to_state, state, layer = open_states.get() # get node from pqueue (and remove from queue)        \n",
    "        dist_to_state = dist_to_state * factor_p_queue # \"re-transform\" if needed\n",
    "\n",
    "        #solve restriction for the subproblem\n",
    "        dd_restricted = build_restricted_dd(dp, instance, layer, state, sink_state, max_width)        \n",
    "        restriction_obj = get_best_objective(dd_restricted)\n",
    "\n",
    "        # see if we improved the best-known feasible solution\n",
    "        if dist_to_state + restriction_obj > best_feasible_obj:            \n",
    "            best_feasible_obj = dist_to_state + restriction_obj\n",
    "           # print (\"new best incumbent\", best_feasible_obj )\n",
    "            \n",
    "        # if the subproblem is exact, no need to continue\n",
    "        if dd_restricted.last_exact_layer == instance.N:\n",
    "            continue\n",
    "        \n",
    "        #solve relaxation for the subproblem       \n",
    "        dd_relaxed = build_relaxed_dd(dp, merge_function, instance, layer, state, sink_state, max_width)        \n",
    "        relaxation_obj = get_best_objective(dd_relaxed) \n",
    "        \n",
    "        # bounding: if cannot improve the best feasible solution, continue\n",
    "        if dist_to_state + relaxation_obj <= best_feasible_obj:\n",
    "            continue\n",
    "    \n",
    "        ## get last_exact_layer cutset and add to open nodes\n",
    "        cutset = dd_relaxed.layers[dd_relaxed.last_exact_layer]    \n",
    "        for state, node_info in  cutset.items():              \n",
    "            open_states.put((factor_p_queue * (dist_to_state + node_info.best_dist), state, dd_relaxed.last_exact_layer))\n",
    "\n",
    "    print (\"open states considered\", number_of_open_states_considered)\n",
    "    return best_feasible_obj\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trying it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#filename = \"./../problems/knapsack/instances/knapPI_1_100_1000_1\" # optimal value: 276457 \n",
    "#values, weights, capacity  = read_knapsack_instance(filename)\n",
    "\n",
    "kp_instance = KPInstance(values, weights,capacity,len(values))\n",
    "filename = \"./../problems/knapsack/instances/knapPI_1_100_1000_1\"\n",
    "kp_instance = read_kp_instance(filename)\n",
    "\n",
    "N = 20\n",
    "kp_instance = KPInstance(kp_instance.values[:N], kp_instance.weights[:N],kp_instance.capacity,  N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open states considered 570\n",
      "CPU times: total: 3.44 s\n",
      "Wall time: 3.59 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9147"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "branch_and_bound_dd(dp_kp, min, kp_instance, 0, 0, -1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of exact DD: 8660\n",
      "CPU times: total: 266 ms\n",
      "Wall time: 272 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9147"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dd = build_exact_dd(dp_kp, kp_instance, 0, 0,-1)\n",
    "print(\"Size of exact DD:\", get_number_of_nodes(dd))\n",
    "get_best_objective(dd)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:audprojekt2022]",
   "language": "python",
   "name": "conda-env-audprojekt2022-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
